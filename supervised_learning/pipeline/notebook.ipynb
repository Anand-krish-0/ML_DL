{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a pipeline in supervised learning is a structured way to automate the workflow for model training and evaluation, ensuring that the sequence of steps is consistently followed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pipeline class in scikit-learn is used to streamline and automate the process of applying a sequence of transformations and an estimator to a dataset. It helps in ensuring that the same sequence of steps is applied consistently during both the training and testing phases, reducing the risk of data leakage and making the code cleaner and more maintainable. Here are the key benefits and uses of using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data leakage</h1>also known as information leakage, refers to a situation where information from outside the training dataset is used to create the model, leading to overly optimistic performance estimates during model evaluation and ultimately resulting in poor generalization to new, unseen data. Data leakage occurs when the model has access to data that it shouldn't have during training, typically because this data won't be available in a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing on Entire Dataset:\n",
    "Performing data preprocessing (like scaling, imputing, or encoding) on the entire dataset before splitting it into training and test sets. This can cause information from the test set to influence the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Examples of Data Leakage</h3>\n",
    "\n",
    "**Example with Feature Derivation**:\n",
    "\n",
    "Suppose you're predicting whether a person will develop diabetes. If one of the features includes future medical records that indicate whether the person developed diabetes, this would leak future information into the training process.\n",
    "\n",
    "\n",
    "**Example with Preprocessing**:\n",
    "When you scale features using the mean and standard deviation of the entire dataset before splitting into training and test sets, the test set statistics influence the training data, leading to data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Classification Report:\\n {classification_report(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multimodel pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Pipeline(steps=[('preprocessing',\n",
      "                 Pipeline(steps=[('imputer', SimpleImputer()),\n",
      "                                 ('scaler', StandardScaler())])),\n",
      "                ('classifier', LogisticRegression(C=1))])\n",
      "\n",
      "Best Parameters: {'classifier__C': 1, 'classifier__solver': 'lbfgs'}\n",
      "\n",
      "Best Score: 0.9583333333333334\n",
      "\n",
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing\n",
    "preprocessing = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define pipelines for each model\n",
    "pipeline_lr = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipeline_svc = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "# Define parameter grids for each pipeline\n",
    "param_grid_lr = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "param_grid_svc = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Combine pipelines and parameter grids\n",
    "pipelines = [\n",
    "    (pipeline_lr, param_grid_lr),\n",
    "    (pipeline_rf, param_grid_rf),\n",
    "    (pipeline_svc, param_grid_svc)\n",
    "]\n",
    "\n",
    "best_models = []\n",
    "for pipeline, param_grid in pipelines:\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models.append((grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_))\n",
    "\n",
    "# Find the best overall model\n",
    "best_model = max(best_models, key=lambda x: x[2])\n",
    "\n",
    "print(f\"Best Model: {best_model[0]}\\n\")\n",
    "print(f\"Best Parameters: {best_model[1]}\\n\")\n",
    "print(f\"Best Score: {best_model[2]}\\n\")\n",
    "\n",
    "# Evaluate the best model on test data\n",
    "y_pred = best_model[0].predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\\n\")\n",
    "print(f\"Classification Report:\\n {classification_report(y_test, y_pred)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline Structure:**\n",
    "\n",
    "The pipeline expects a fixed sequence of steps. By placing a classifier step with any initial classifier (in this case, LogisticRegression()), we create a valid pipeline structure that can later be modified dynamically during the grid search process.\n",
    "\n",
    "**Dynamic Replacement in Grid Search:**\n",
    "\n",
    "When GridSearchCV runs, it systematically replaces the classifier step with each classifier specified in the param_grid. The grid search process involves:\n",
    "\n",
    "Replacing the classifier step with each candidate model (e.g., LogisticRegression(), RandomForestClassifier(), SVC()).\n",
    "Setting the specified hyperparameters for each candidate model.\n",
    "Evaluating the performance of each configuration through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
