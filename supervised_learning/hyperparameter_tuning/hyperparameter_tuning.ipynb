{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is a crucial aspect of building machine learning models as it involves finding the optimal set of hyperparameters that maximize the model's performance. Here are some common techniques used for hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search: Grid Search is a brute-force technique that involves defining a grid of hyperparameters and exhaustively searching all possible combinations of hyperparameters. It evaluates each combination using cross-validation and selects the combination that yields the best performance.\n",
    "\n",
    "Random Search: Unlike Grid Search, Random Search randomly samples hyperparameters from specified distributions. It doesn't exhaustively search through all possible combinations but rather explores a larger portion of the hyperparameter space more efficiently. Random Search is particularly useful when the hyperparameter space is large and it's not feasible to explore all combinations.\n",
    "\n",
    "Bayesian Optimization: Bayesian Optimization is an iterative optimization technique that builds a probabilistic model of the objective function (model performance) and uses this model to decide where to sample the next set of hyperparameters. It balances exploration (sampling in regions with uncertain outcomes) and exploitation (sampling in regions likely to yield good performance) to efficiently find the optimal set of hyperparameters.\n",
    "\n",
    "Gradient-based Optimization: Gradient-based optimization techniques, such as gradient descent or its variants, can be used to optimize hyperparameters. In this approach, the objective function (model performance) is treated as a differentiable function of hyperparameters, and gradients with respect to the hyperparameters are computed to iteratively update them towards the optimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search Cross-Validation combines Grid Search, a hyperparameter optimization technique, with Cross-Validation to systematically search through a specified grid of hyperparameters and select the best combination that yields the highest model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search: Grid Search involves defining a grid of hyperparameters that you want to optimize. For example, if you're training a Support Vector Machine (SVM), you might want to optimize the kernel type and the value of the regularization parameter (C). You specify a range of values for each hyperparameter.\n",
    "\n",
    "Cross-Validation: With Cross-Validation, you partition the dataset into training and validation sets. Then, you train the model on the training set and evaluate it on the validation set. This process is repeated multiple times with different partitions of the data, and the average performance is computed.\n",
    "\n",
    "Grid Search with Cross-Validation: In Grid Search Cross-Validation, for each combination of hyperparameters in the grid:\n",
    "    The dataset is split into training and validation sets.\n",
    "    The model is trained on the training set.\n",
    "    The performance of the model is evaluated on the validation set using a specified metric (e.g., accuracy, F1 score).\n",
    "    This process is repeated for each fold in the Cross-Validation.\n",
    "    The average performance across all folds is calculated for each hyperparameter combination.\n",
    "\n",
    "Selecting the Best Model: After evaluating all combinations of hyperparameters, the combination that results in the highest average performance across all folds is selected as the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Grid Search Cross-Validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Systematic Exploration: Grid Search systematically explores the hyperparameter space, testing different combinations to find the optimal set.\n",
    "\n",
    "Prevents Overfitting: By using Cross-Validation, Grid Search ensures that the hyperparameters selected generalize well to unseen data.\n",
    "\n",
    "Optimal Model Selection: Grid Search helps in selecting the best model configuration based on performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks of Grid Search Cross-Validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Computational Cost: Grid Search Cross-Validation can be computationally expensive, especially if the hyperparameter space is large or the dataset is large.\n",
    "\n",
    "Exhaustive Search: Grid Search explores only the specified combinations in the grid and may miss out on potentially better hyperparameter configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "Best Accuracy: 0.9583333333333334\n",
      "Test Set Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid to search through\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1], 'kernel': ['rbf', 'linear']}\n",
    "\n",
    "# Instantiate the Support Vector Machine classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object with the SVM classifier and parameter grid\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_score)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Test Set Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the iris dataset and split it into training and testing sets.\n",
    "\n",
    "We define a parameter grid for the Support Vector Machine (SVM) classifier, specifying different values for the regularization parameter 'C', the kernel coefficient 'gamma', and the kernel type 'kernel'.\n",
    "\n",
    "We instantiate the SVM classifier.\n",
    "\n",
    "We create a GridSearchCV object, passing the SVM classifier, parameter grid, number of folds for cross-validation (cv=5), and the scoring metric ('accuracy' in this case).\n",
    "\n",
    "We fit the GridSearchCV object to the training data, which will perform cross-validation and search through the parameter grid to find the best combination of hyperparameters.\n",
    "\n",
    "We retrieve the best parameters and best score from the grid search results.\n",
    "Finally, we evaluate the best model on the test set to assess its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Random Search is another hyperparameter optimization technique used in machine learning, similar to Grid Search. However, instead of systematically searching through a predefined grid of hyperparameters, Random Search selects hyperparameter combinations at random from a specified search space.\n",
    "\n",
    "Here's how Random Search works:\n",
    "\n",
    "1) Define Search Space: You specify a search space for each hyperparameter, which can be a range of values, a list of discrete values, or a distribution from which values are sampled.\n",
    "\n",
    "2) Randomly Sample Hyperparameters: Random Search randomly selects combinations of hyperparameters from the defined search space.\n",
    "\n",
    "3) Evaluate Performance: Each combination of hyperparameters is evaluated using cross-validation or a separate validation set to estimate the model's performance.\n",
    "\n",
    "5) Select Best Model: After evaluating a predefined number of combinations (or until a stopping criterion is met), the combination that yields the best performance metric is selected as the optimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of Random Search:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficient Exploration: Random Search efficiently explores the hyperparameter space by sampling from it randomly, potentially covering a wider range of values compared to Grid Search.\n",
    "\n",
    "Computationally Less Expensive: Random Search is often less computationally expensive than Grid Search, especially when the hyperparameter space is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disadvantages of Random Search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Systematic Exploration: Unlike Grid Search, Random Search does not systematically explore the hyperparameter space, which may result in missing out on optimal combinations.\n",
    "\n",
    "May Require More Evaluations: Random Search may require more evaluations (i.e., training and testing models) to find the optimal combination, especially if the search space is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 3.845401188473625, 'gamma': 0.9517143064099162, 'kernel': 'rbf'}\n",
      "Best Accuracy: 0.9583333333333334\n",
      "Test Set Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter distributions to sample from\n",
    "param_distributions = {\n",
    "    'C': uniform(0.1, 10),        # Continuous uniform distribution between 0.1 and 10\n",
    "    'gamma': uniform(0.001, 1),   # Continuous uniform distribution between 0.001 and 1\n",
    "    'kernel': ['rbf', 'linear']   # Discrete choices for the kernel\n",
    "}\n",
    "\n",
    "# Instantiate the Support Vector Machine classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object with the SVM classifier and parameter distributions\n",
    "random_search = RandomizedSearchCV(svm, param_distributions, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Fit the RandomizedSearchCV object to the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_score)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Test Set Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define parameter distributions for the Support Vector Machine (SVM) classifier, specifying ranges for 'C' and 'gamma' as continuous uniform distributions, and a list of choices for 'kernel'.\n",
    "\n",
    "We instantiate the RandomizedSearchCV object, specifying the SVM classifier, parameter distributions, number of iterations (n_iter=10), number of folds for cross-validation (cv=5), scoring metric ('accuracy'), and a random state for reproducibility.\n",
    "\n",
    "We fit the RandomizedSearchCV object to the training data, which will randomly sample combinations of hyperparameters from the specified distributions and evaluate them using cross-validation.\n",
    "\n",
    "We retrieve the best parameters and best score from the random search results.\n",
    "Finally, we evaluate the best model on the test set to assess its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
